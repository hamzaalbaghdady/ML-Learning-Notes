# -*- coding: utf-8 -*-
"""Pandas project 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E61wRufduu6c6F-aQIAxcKALZlp5-VhA

# Walmart Store Sales Forecasting Dataset

### Getting started

#### Importing...
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files

"""#### Load Dataset"""

train = pd.read_csv("train.csv")
features = pd.read_csv("features.csv")
stores = pd.read_csv("stores.csv")

# Convert date columns
train['Date'] = pd.to_datetime(train['Date'], errors='coerce')
features['Date'] = pd.to_datetime(features['Date'], errors='coerce')
# convert before merge to avoid data mismatch: for example 11-11-1111 : 11/11/1111

# Merge datasets
df = train.merge(features, on=['Store', 'Date'], how='left')
df = df.merge(stores, on='Store', how='left')

# # Handle missing values
# df.loc[['CPI','Fuel_Price']]fillna(method='ffill', inplace=True)
# # ffill: Forward Fill: Fills missing values by copying the last known value forward.
# #Why use it here? Economic indicators like CPI or fuel price don’t change daily. Forward fill assumes the last value is still valid until updated.

# Create new features
df['Month'] = df['Date'].dt.month
df['Year'] = df['Date'].dt.year
df['Week'] = df['Date'].dt.isocalendar().week
# Helps with filtering data and with groupby

df.head()
# Normalize holiday flag
df['IsHoliday_y'] = df['IsHoliday_y'].astype(int)
# Converts value from bool to int which makes it easier to use in numeric operations, visualizations, or machine learning models.

# merging error => fix:
df['IsHoliday'] = df['IsHoliday_y']  # or 'IsHoliday_x'
df.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1, inplace=True)

"""### Have a look on the data"""

df.head()

"""**Store**	Store ID

**Dept**	Department number	Each store has multiple departments.

**Date**	Week-ending date

**Weekly_Sales**	Sales for that store-department-week

**Temperature**	Average temperature that week (local)	External factor.

**Fuel_Price**	Local fuel price	Economic indicator.

MarkDown1–5	Promotional markdowns (discounts)	Represent different types of
promotions.

**CPI**	Consumer Price Index	Macro-economic indicator.

**Unemployment**	Local unemployment rate	Another economic indicator. Higher unemployment may reduce sales.

**Type**	Store type (A, B, C)	Categorical feature. Larger stores (Type A) may have higher sales.

**Size**	Store size in square feet	Numeric feature. Bigger stores → more capacity, potentially higher sales.

**Month**	Extracted from Date,
**Year**	Extracted from Date,	Useful for trend analysis across years.
**Week**	ISO week number	Captures weekly seasonality. Important for aligning across years.

**IsHoliday**	Binary flag (1 = holiday week, 0 = non-holiday)	Critical feature. Sales spike during holidays. Must be handled carefully in forecasting.
"""

df.info()

df.describe(exclude=['datetime64','UInt32','int32'])

df.loc[:,['Weekly_Sales','Temperature','Fuel_Price','CPI','Unemployment']].describe()

"""Weekly_Sales has a high std; that means data has outliers"""

df.isna().sum()

"""We have a lot missing values in markDowns"""

df.nunique()

"""I can see that we are dealing with **45** stores, **81** department, **3** store types, and **3** years."""

df['Date'].agg(['min','max'])

"""The sales starts at 5 Feb 2010 till 26 Oct 2012 that almost 3 years."""

df.groupby('Store').size().sort_values(ascending=False)

"""We can say that store 13 has the most sales operations unlike 36 which has the least."""

df.groupby('Dept').size().sort_values(ascending=False)

"""We can say that department 1 has the most sales operations unlike 43 which has the least."""

summary = pd.DataFrame({
    "dtype": df.dtypes,
    "unique_values": df.nunique(),
    "missing_values": df.isna().sum(),
    "total_rows": len(df)
})

summary["missing_pct"] = (summary["missing_values"] / summary["total_rows"]) * 100
summary

df.groupby('Store')['Dept'].nunique()

"""All stores has 60-80 department

### Fix, validate, and clean data
"""

# markdowns is a different promotional strategies Walmart used during the dataset period.
#Each MarkDown column is a numeric value (dollar amount) representing discounts or promotions applied to products in a given week.
df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']] = \
    df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].fillna(0)

df['Total_MarkDown'] = df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].sum(axis=1)

summary

"""## Understanding data more"""

df.head(10)

"""#### Dates
It's all about time honey.
"""

df.groupby('Year')[['Weekly_Sales','CPI','Unemployment','Fuel_Price','Temperature','Total_MarkDown']].sum()/1e6

"""We can see that there is a relation between this data,
MarkDowns increase makes sales decrease
Temperature, CPI, unemployment, and fuel effect sales.
"""

df.groupby(['Year','Month'])[['Weekly_Sales','CPI','Unemployment','Fuel_Price','Temperature','Total_MarkDown']]\
.sum().sort_values(by='Weekly_Sales', ascending=False)

"""The highest sales happen in December because holidays I can say."""

# Step 1: Group and aggregate
date_feature = df.groupby(['Year', 'Month'])[
    ['Weekly_Sales', 'CPI', 'Unemployment', 'Fuel_Price', 'Temperature', 'Total_MarkDown']
].mean().reset_index()

# Step 2: Create a proper datetime column
date_feature['Date'] = pd.to_datetime(date_feature[['Year', 'Month']].assign(DAY=1))

# Step 3: Set plot style
sns.set(style="whitegrid")
fig, axes = plt.subplots(3, 2, figsize=(16, 12), sharex=True)
fig.suptitle("Monthly Trends in Sales and Economic Indicators", fontsize=16)

# Step 4: Plot each feature
features = ['Weekly_Sales', 'CPI', 'Unemployment', 'Fuel_Price', 'Temperature', 'Total_MarkDown']
for ax, feature in zip(axes.flat, features):
    sns.lineplot(data=date_feature, x='Date', y=feature, ax=ax)
    ax.set_title(feature.replace('_', ' '))
    ax.set_ylabel('')
    ax.tick_params(axis='x', rotation=45)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

"""The highest sales are in December which i belive because holidays, and the lowest ones in January,
MarkDowns have no relation with sales rate.
Other features need more studing.
"""

from sklearn.preprocessing import MinMaxScaler

# Step 1: Group and aggregate monthly data
date_feature = df.groupby(['Year', 'Month'])[
    ['Weekly_Sales', 'CPI', 'Unemployment', 'Fuel_Price', 'Temperature', 'Total_MarkDown']
].sum().reset_index()

# Step 2: Create a proper datetime column
date_feature['Date'] = pd.to_datetime(date_feature[['Year', 'Month']].assign(DAY=1))

# Step 3: Normalize all metric columns using MinMaxScaler
scaler = MinMaxScaler()
scaled_values = scaler.fit_transform(date_feature[
    ['Weekly_Sales', 'CPI', 'Unemployment', 'Fuel_Price', 'Temperature', 'Total_MarkDown']
])
scaled_df = pd.DataFrame(scaled_values, columns=[
    'Weekly_Sales', 'CPI', 'Unemployment', 'Fuel_Price', 'Temperature', 'Total_MarkDown'
])
scaled_df['Date'] = date_feature['Date']

# Step 4: Melt the DataFrame for seaborn
melted = scaled_df.melt(id_vars='Date', var_name='Metric', value_name='Value')

# Step 5: Plot all metrics on one chart
plt.figure(figsize=(14, 6))
sns.lineplot(data=melted, x='Date', y='Value', hue='Metric')
plt.title("Monthly Trends: Normalized Sales & Economic Indicators")
plt.xlabel("Date")
plt.ylabel("Normalized Value (0–1)")
plt.xticks(rotation=45)
plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Step 1: Group and aggregate monthly data
date_feature = df.groupby(['Year', 'Month'])[
    ['Weekly_Sales', 'CPI', 'Unemployment', 'Fuel_Price', 'Temperature', 'Total_MarkDown']
].sum().reset_index()

# Step 2: Create a proper datetime column
date_feature['Date'] = pd.to_datetime(date_feature[['Year', 'Month']].assign(DAY=1))

# Step 3: Normalize all metric columns using MinMaxScaler
scaler = MinMaxScaler()
scaled_values = scaler.fit_transform(date_feature[
    ['Weekly_Sales', 'CPI', 'Unemployment', 'Fuel_Price', 'Temperature', 'Total_MarkDown']
])
scaled_df = pd.DataFrame(scaled_values, columns=[
    'Weekly_Sales', 'CPI', 'Unemployment', 'Fuel_Price', 'Temperature', 'Total_MarkDown'
])
scaled_df['Date'] = date_feature['Date']
scaled_df['Year'] = date_feature['Year']

# Step 4: Melt the DataFrame for seaborn
melted = scaled_df.melt(id_vars=['Date', 'Year'], var_name='Metric', value_name='Value')

# Step 5: Plot each year in a separate full-width figure
for year in [2010, 2011, 2012]:
    plt.figure(figsize=(16, 6))
    sns.lineplot(data=melted[melted['Year'] == year], x='Date', y='Value', hue='Metric', palette='tab10')
    plt.title(f"Normalized Monthly Trends – {year}")
    plt.xlabel("Date")
    plt.ylabel("Normalized Value (0–1)")
    plt.xticks(rotation=45)
    plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.show()

"""We can rely on CPI, fuel, and unemployment to predict sales. Temperature not too much. MarkDowns not reliable"""

sns.lineplot(data=df,x='Date',y='Weekly_Sales')
plt.title("Weekly Sales Trends (2010–2012)")
plt.xlabel("Date")
plt.ylabel("Weekly Sales")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

filtered_df = df[df['Year'].isin([2010, 2011, 2012])]

# Plot all years on one chart
plt.figure(figsize=(14, 6))
sns.lineplot(data=filtered_df,x='Date',y='Weekly_Sales',hue='Year', palette='tab10')
plt.title("Weekly Sales Trends (2010–2012)")
plt.xlabel("Date")
plt.ylabel("Weekly Sales")
plt.xticks(rotation=45)
plt.legend(title='Year')
plt.tight_layout()
plt.show()

sns.lineplot(data=df,x='Month',y='Weekly_Sales')
plt.title("Weekly Sales Trends (2010–2012)")
plt.xlabel("Month")
plt.ylabel("Weekly Sales")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

sns.lineplot(data=df, x='Date', y='Weekly_Sales', hue='IsHoliday', palette='Set1')
plt.title("Weekly Sales Trends (2010–2012)")
plt.xlabel("Month")
plt.ylabel("Weekly Sales")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

sns.lineplot(data=df.loc[df.Year==2011], x='Date', y='Weekly_Sales', hue='IsHoliday', palette='Set1')
plt.title("Weekly Sales Trends 2011")
plt.xlabel("Month")
plt.ylabel("Weekly Sales")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""#### Features"""

# Set plot style
sns.set(style="whitegrid")

# Create figure with subplots
fig, axes = plt.subplots(5, 2, figsize=(14, 12))
fig.suptitle("Scatter Plots: Weekly Sales vs Economic Indicators", fontsize=16)

# Define pairs to plot
pairs = [
    ('Total_MarkDown', 'Sales vs Total MarkDown'),
    ('CPI', 'Sales vs CPI'),
    ('Unemployment', 'Sales vs Unemployment'),
    ('Temperature', 'Sales vs Temperature'),
    ('Fuel_Price', 'Sales vs Fuel Price'),
    ('Size', 'Sales vs store size'),
    ('Type', 'Sales vs store type'),
    ('Store', 'Sales vs store'),
    ('Dept', 'Sales vs department'),
    ('IsHoliday', 'Sales vs holidays'),

]

# Plot each pair
for ax, (feature, title) in zip(axes.flat, pairs):
    sns.scatterplot(data=df, x=feature, y='Weekly_Sales', ax=ax, alpha=0.5)
    ax.set_title(title)
    ax.set_xlabel(feature)
    ax.set_ylabel('Weekly Sales')

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

"""**Direct Relationship:**

1.   CPI and Sales
2.   Store size and Sales
3.   Type and Sales
4.   Holidays and Sales

**Inverse Relationship:**

1.   MarksDown and Sales
2.   Unemployment and Sales

**No Relations At All:**

1.   Tempretuer and Sales
2.   Fuel and Sales

**Unclear Relations:**

1. Sales vs. Store
2. Sales vs. Department
"""

# Set plot style
sns.set(style="whitegrid")

# Create figure with subplots
fig, axes = plt.subplots(5, 2, figsize=(14, 12))
fig.suptitle("Scatter Plots: Weekly Sales vs Economic Indicators", fontsize=16)

# Define pairs to plot
pairs = [
    ('Total_MarkDown', 'Sales vs Total MarkDown'),
    ('CPI', 'Sales vs CPI'),
    ('Unemployment', 'Sales vs Unemployment'),
    ('Temperature', 'Sales vs Temperature'),
    ('Fuel_Price', 'Sales vs Fuel Price'),
    ('Size', 'Sales vs store size'),
    ('Type', 'Sales vs store type'),
    ('Store', 'Sales vs store'),
    ('Dept', 'Sales vs department'),
    ('IsHoliday', 'Sales vs holidays'),

]

# Plot each pair
for ax, (feature, title) in zip(axes.flat, pairs):
    sns.lineplot(data=df, x=feature, y='Weekly_Sales', ax=ax, alpha=0.5)
    ax.set_title(title)
    ax.set_xlabel(feature)
    ax.set_ylabel('Weekly Sales')

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

"""Fuel price has a Direct relationship with sales

#### Departments
"""

depts=df.groupby('Dept')[['Store','Size','Type','Weekly_Sales']].value_counts().reset_index().sort_values(by='Weekly_Sales',ascending=False)
depts

sns.lineplot(data=depts, x='Dept', y='Weekly_Sales', hue='Size', palette='Set1')
plt.title("Weekly Sales Trends among Departments")
plt.xlabel("Departments")
plt.ylabel("Weekly Sales")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

sns.lineplot(data=depts, x='Dept', y='Weekly_Sales', hue='Type', palette='Set1')
plt.title("Weekly Sales Trends among Departments")
plt.xlabel("Departments")
plt.ylabel("Weekly Sales")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df.groupby('Type')['Dept'].value_counts()

g = sns.FacetGrid(depts, col='Store', col_wrap=5, height=3, sharey=False)
g.map(sns.lineplot, 'Dept', 'Weekly_Sales')
g.set_titles("Store {col_name}")
g.set_axis_labels("Department", "Weekly Sales")
plt.tight_layout()
plt.show()

df.groupby('Store')['Dept'].value_counts()

sns.barplot(data=depts, x='Store', y='Weekly_Sales')
plt.title("Weekly Sales Trends among Departments")
plt.xlabel("Stores")
plt.ylabel("Weekly Sales")
plt.tight_layout()
plt.show()

sns.barplot(data=depts, x='Dept', y='Weekly_Sales')
plt.title("Weekly Sales Trends among Departments")
plt.xlabel("Departments")
plt.ylabel("Weekly Sales")
plt.tight_layout()
plt.show()

dept_avg=df.groupby('Dept')['Weekly_Sales'].mean().sort_values(ascending=False).reset_index()
dept_avg

sns.heatmap(data=dept_avg)
plt.title("Weekly Sales Trends among Departments")
plt.xlabel("Departments")
plt.ylabel("Weekly Sales")
plt.tight_layout()
plt.show()

"""### Dive Deeper"""

df.groupby('Week')['Weekly_Sales'].mean().plot(figsize=(10,4))

store_sales = df.groupby(['Store','Date'])['Weekly_Sales'].sum().unstack()
sns.clustermap(store_sales.fillna(0), cmap="viridis")

store_sales = df.groupby(['Dept','Date'])['Weekly_Sales'].sum().unstack()
sns.clustermap(store_sales.fillna(0), cmap="viridis")

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plot_acf(df['Weekly_Sales'])
plot_pacf(df['Weekly_Sales'])

# Pivot table: sum of weekly sales by Store and Dept
pivot = df.pivot_table(index='Store', columns='Dept', values='Weekly_Sales', aggfunc='sum', fill_value=0)

plt.figure(figsize=(16, 8))
sns.heatmap(pivot, cmap='YlGnBu')
plt.title("Store vs Department Weekly Sales Heatmap")
plt.xlabel("Department")
plt.ylabel("Store")
plt.show()

g = sns.FacetGrid(df, col='Store', col_wrap=5, height=3, sharey=False)
g.map(sns.lineplot, 'Dept', 'Weekly_Sales')
g.set_titles("Store {col_name}")
g.set_axis_labels("Department", "Weekly Sales")
plt.tight_layout()
plt.show()

# Add season feature
df['Season'] = df['Month']%12 // 3
season_map = {0:'Winter', 1:'Spring', 2:'Summer', 3:'Fall'}
df['Season'] = df['Season'].map(season_map)

# Aggregate markdowns by store type and season
df.groupby(['Type','Season'])['Total_MarkDown'].mean().reset_index()

# Create lag features (shift by 1 week)
df = df.sort_values(['Store','Dept','Date'])
df['Weekly_Sales_lag1'] = df.groupby(['Store','Dept'])['Weekly_Sales'].shift(1)
df['CPI_lag1'] = df.groupby('Store')['CPI'].shift(1)
df['Fuel_Price_lag1'] = df.groupby('Store')['Fuel_Price'].shift(1)
# These lag features allow models to learn from previous week’s conditions.

df.head(10)

# Save DataFrame to CSV
df.to_csv("walmart_cleaned.csv", index=False)

# Download the file
from google.colab import files
files.download("walmart_cleaned.csv")